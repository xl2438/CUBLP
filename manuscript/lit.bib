@article{Fill1999,
abstract = {In this paper we exhibit, under suitable conditions, a neat relationship between the Moore-Penrose generalized inverse of a sum of two matrices and the Moore-Penrose generalized inverses of the individual terms. We include an application to the parallel sum of matrices.},
author = {Fill, James Allen and Fishkind, Donniell E.},
doi = {10.1137/s0895479897329692},
issn = {08954798},
journal = {SIAM Journal on Matrix Analysis and Applications},
keywords = {Moore-Penrose generalized inverse,Parallel sum,Rank additivity,Sherman-Morrison-Woodbury formula,Singular value decomposition},
number = {2},
pages = {629--635},
title = {{The Moore-Penrose generalized inverse for sums of matrices}},
volume = {21},
year = {1999}
}
@article{Robinson1991,
abstract = {In animal breeding, Best Linear Unbiased Prediction, or BLUP, is a technique for estimating genetic merits. In general, it is a method of estimating random effects. It can be used to derive the Kalman filter, the method of Kriging used for ore reserve estimation, credibility theory used to work out insurance premiums, and Hoadley's quality measurement plan used to estimate a quality index. It can be used for removing noise from images and for small-area estimation. This paper presents the theory of BLUP, some examples of its application and its relevance to the foundations of statistics. Understanding of procedures for estimating random effects should help people to understand some complicated and controversial issues about fixed and random effects models and also help to bridge the apparent gulf between the Bayesian and Classical schools of thought. {\textcopyright} 1991, Institute of Mathematical Statistics. All Rights Reserved.},
author = {Robinson, G. K.},
doi = {10.1214/ss/1177011926},
file = {:home/xiang/Downloads/2245695.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
number = {1},
pages = {15--32},
title = {{That BLUP is a good thing: The estimation of random effects}},
volume = {6},
year = {1991}
}
@article{Zhang2015,
abstract = {In educational measurement contexts, essays have been evaluated and formative feedback has been given based on the end product. In this study, we used a large sample collected from middle school students in the United States to investigate the factor structure of the writing process features gathered from keystroke logs and the association of that latent structure with the quality of the final product (i.e., the essay text). The extent to which those process factors had incremental value over product features was also examined. We extracted 29 process features using the keystroke logging engine developed at Educational Testing Service (ETS). We identified 4 factors that represent the extent of writing fluency, local word-level editing, phrasal/chunk-level editing, and planning and deliberation during writing. We found that 2 of the 4 factors—writing fluency, and planning and deliberation—significantly related to the quality of the final text, whereas the 4 factors altogether accounted for limited variance in human scores. In 1 of the 2 samples studied, the keystroke-logging fluency factor added incrementally, but only marginally, to the prediction of human ratings of text-production skills beyond product features. The limited power of the writing process features for predicting human scores and the lack of clear additional predictive value over product features are not surprising given that the human raters have no knowledge of the writing process leading to the final text and that the product features measure the basic text quality specified in the human scoring rubric. Study limitations and recommendation for future research are also provided.},
author = {Zhang, Mo and Deane, Paul},
doi = {10.1002/ets2.12075},
journal = {ETS Research Report Series},
number = {2},
pages = {1--12},
title = {{Process Features in Writing: Internal Structure and Incremental Value Over Product Features}},
volume = {2015},
year = {2015}
}
@article{Burstein2004,
abstract = {In this article, we describe a deployed educational technology application: the Criterion Online Essay Evaluation Service, a web-based system that provides automated scoring and evaluation of student essays. Criterion has two complementary applications: (1) Critique Writing Analysis Tools, a suite of programs that detect errors in grammar, usage, and mechanics, that identify discourse elements in the essay, and that recognize potentially undesirable elements of style, and (2) e-rater version 2.0, an automated essay scoring system. Critique and e-rater provide students with feedback that is specific to their writing in order to help them improve their writing skills and is intended to be used under the instruction of a classroom teacher. Both applications employ natural language processing and machine learning techniques. All of these capabilities outperform baseline algorithms, and some of the tools agree with human judges in their evaluations as often as two judges agree with each other. {\textcopyright} 2004, American Association for Artificial Intelligence. All rights reserved.},
author = {Burstein, Jill and Chodorow, Martin and Leacock, Claudia},
issn = {07384602},
journal = {AI Magazine},
number = {3},
pages = {27--36},
title = {{Automated essay evaluation: The criterion online writing service}},
volume = {25},
year = {2004}
}
@misc{Attali2006,
abstract = {E-rater{\textregistered} has been used by the Educational Testing Service for automated essay scoring since 1999. This paper describes a new version of e-rater (V.2) that is different from other automated essay scoring systems in several important respects. The main innovations of e-rater V.2 are a small, intuitive, and meaningful set of features used for scoring; a single scoring model and standards can be used across all prompts of an assessment; modeling procedures that are transparent and flexible, and can be based entirely on expert judgment. The paper describes this new system and presents evidence on the validity and reliability of its scores. Copyright {\textcopyright} 2006 by the Journal of Technology, Learning, and Assessment.},
author = {Attal{\'{i}}, Yigal and Burste{\'{i}}n, Jill},
booktitle = {Journal of Technology, Learning, and Assessment},
doi = {10.1002/j.2333-8504.2004.tb01972.x},
issn = {15402525},
number = {3},
pages = {1--29},
title = {{Automated essay scoring with e-rater{\textregistered} V.2}},
volume = {4},
year = {2006}
}
@article{Zhang2019,
abstract = {This study compared gender groups on the processes used in writing essays in an online assessment. Middle-school students from four grades responded to essays in two persuasive subgenres, argumentation and policy recommendation. Writing processes were inferred from four indicators extracted from students' keystroke logs. In comparison to males, on average females not only obtained higher essay scores but differed from males in their writing processes. Females entered text more fluently, engaged in more macro and local editing, and showed less need to pause at locations associated with planning (e.g., between bursts of text, at sentence boundaries). That these differences were detected after controlling for essay scores suggests that they cannot be attributed solely to disparities in group writing skill.},
author = {Zhang, Mo and Bennett, Randy E. and Deane, Paul and van Rijn, Peter W.},
doi = {10.1111/emip.12249},
issn = {17453992},
journal = {Educational Measurement: Issues and Practice},
keywords = {gender differences,problem-solving processes,scenario-based assessment,writing},
number = {2},
pages = {14--26},
title = {{Are There Gender Differences in How Students Write Their Essays? An Analysis of Writing Processes}},
volume = {38},
year = {2019}
}
@article{Sinharay2019,
abstract = {Analysis of keystroke logging data is of increasing interest, as evident from a substantial amount of recent research on the topic. Some of the research on keystroke logging data has focused on the prediction of essay scores from keystroke logging features, but linear regression is the only prediction method that has been used in this research. Data mining methods such as boosting and random forests have been found to improve over traditional prediction methods such as linear regression in various scientific fields, but have not been used in the prediction of essay scores from keystroke logging features. This article first provides a review of boosting, which is a popular data mining method. The article then applies boosting to predict essay scores from a large number of keystroke logging features and other predictor variables from two real data sets.},
author = {Sinharay, Sandip and Zhang, Mo and Deane, Paul},
doi = {10.1080/08957347.2019.1577245},
file = {:home/xiang/Downloads/document(3).pdf:pdf},
issn = {08957347},
journal = {Applied Measurement in Education},
number = {2},
pages = {116--137},
title = {{Prediction of Essay Scores From Writing Process and Product Features Using Data Mining Methods}},
volume = {32},
year = {2019}
}
@article{Henderson1975,
abstract = {Mixed linear models are assumed in most animal breeding applications. Convenient methods for computing BLUE of the estimable linear functions of the fixed elements of the model and for computing best linear unbiased predictions of the random elements of the model have been available. Most data available to animal breeders, however, do not meet the usual requirements of random sampling, the problem being that the data arise either from selection experiments or from breeders' herds which are undergoing selection. Consequently, the usual methods are likely to yield biased estimates and predictions. Methods for dealing with such data are presented in this paper.},
author = {Henderson, C. R.},
doi = {10.2307/2529430},
file = {:home/xiang/Downloads/2529430.pdf:pdf},
issn = {0006341X},
journal = {Biometrics},
month = {jun},
number = {2},
pages = {423},
pmid = {1174616},
publisher = {JSTOR},
title = {{Best Linear Unbiased Estimation and Prediction under a Selection Model}},
volume = {31},
year = {1975}
}
@article{Haberman2015,
abstract = {In many educational tests which involve constructed responses, a traditional test score is obtained by adding together item scores obtained through holistic scoring by trained human raters. For example, this practice was used until 2008 in the case of GRE<sup>{\textregistered}</sup>General Analytical Writing and until 2009 in the case of TOEFL<sup>{\textregistered}</sup> iBT Writing. With use of natural language processing, it is possible to obtain additional information concerning item responses from computer programs such as e-rater<sup>{\textregistered}</sup>. In addition, available information relevant to examinee performance may include scores on related tests. We suggest application of standard results from classical test theory to the available data to obtain best linear predictors of true traditional test scores. In performing such analysis, we require estimation of variances and covariances of measurement errors, a task which can be quite difficult in the case of tests with limited numbers of items and with multiple measurements per item. As a consequence, a new estimation method is suggested based on samples of examinees who have taken an assessment more than once. Such samples are typically not random samples of the general population of examinees, so that we apply statistical adjustment methods to obtain the needed estimated variances and covariances of measurement errors. To examine practical implications of the suggested methods of analysis, applications are made to GRE General Analytical Writing and TOEFL iBT Writing. Results obtained indicate that substantial improvements are possible both in terms of reliability of scoring and in terms of assessment reliability.},
author = {Haberman, Shelby J. and Yao, Lili and Sinharay, Sandip},
doi = {10.1111/bmsp.12052},
file = {:home/xiang/Downloads/document(1).pdf:pdf},
issn = {20448317},
journal = {British Journal of Mathematical and Statistical Psychology},
keywords = {Adjustment,Augmentation,Automated scoring,Classical test theory,Holistic scoring,Proportional reduction in mean square error,Reliability},
number = {2},
pages = {363--385},
pmid = {25773314},
title = {{Prediction of true test scores from observed item scores and ancillary data}},
volume = {68},
year = {2015}
}
@article{Yao2019,
abstract = {In best linear prediction (BLP), a true test score is predicted by observed item scores and by ancillary test data. If the use of BLP rather than a more direct estimate of a true score has disparate impact for different demographic groups, then a fairness issue arises. To improve population invariance but to preserve much of the efficiency of BLP, a modified approach, penalized best linear prediction, is proposed that weights both mean square error of prediction and a quadratic measure of subgroup biases. The proposed methodology is applied to three high-stakes writing assessments.},
author = {Yao, Lili and Haberman, Shelby J. and Zhang, Mo},
doi = {10.1007/s11336-018-9636-7},
file = {:home/xiang/Downloads/document.pdf:pdf},
issn = {00333123},
journal = {Psychometrika},
keywords = {PBLP,subgroup biases,true test score},
number = {1},
pages = {186--211},
pmid = {30242609},
title = {{Penalized Best Linear Prediction of True Test Scores}},
volume = {84},
year = {2019}
}
@article{Guo2019,
abstract = {We used an unobtrusive approach, keystroke logging, to examine students' cognitive states during essay writing. Based on data contained in the logs, we classified writing process data into three states: text production, long pause, and editing. We used semi-Markov processes to model the sequences of writing states and compared the state transition time and probability for demographic subgroups that were matched on writing proficiency. Results suggested that the subgroups employed different processes in essay writing.},
author = {Guo, Hongwen and Zhang, Mo and Deane, Paul and Bennett, Randy E.},
doi = {10.3102/1076998619856590},
issn = {10769986},
journal = {Journal of Educational and Behavioral Statistics},
keywords = {process data,semi-Markov model,writing proficiency},
month = {jun},
number = {5},
pages = {571--596},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Writing Process Differences in Subgroups Reflected in Keystroke Logs}},
url = {http://journals.sagepub.com/doi/10.3102/1076998619856590},
volume = {44},
year = {2019}
}
